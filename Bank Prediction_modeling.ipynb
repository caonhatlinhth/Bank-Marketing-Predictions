{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "df_bank = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"Total row and column in the data set is:\", df_bank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PREPROCESSING**\n",
    "- Write a function for preprocessing step so that we can use this function for both train dataset and test dataset.\n",
    "- Use StandardScaler to standardize numerical variables and using PCA to transfomr these variables. In this case, we did not use PCA to reduce dimensionality of numerical variables. These step increase the performance of the model compared to \"not using PCA\" and \"using PCA to reduce dimensionality.\n",
    "- Use get_dummies to one-hot encode categorical variables\n",
    "- Consider preprocessing steps in test data such as return **id** column in prediction file when use trained model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, transforms=None, has_label=True, return_id=False):\n",
    "    id = df.loc[:, \"id\"]\n",
    "    # df = df.drop(columns = ['id', 'month', 'day', 'civil', 'employees'])\n",
    "    df = df.drop(columns = [\"id\", \"day\", \"employees\"])\n",
    "    if has_label:\n",
    "        X = df.drop(columns = \"outcome\")\n",
    "    else:\n",
    "        X = df\n",
    "\n",
    "    non_number_cols = [k for k in X.keys() if str(X[k].dtype) not in [\"int64\", \"float64\"]]\n",
    "    number_cols = list(set(X.keys()) - set(non_number_cols))\n",
    "    X_numeric = X.loc[:, number_cols]\n",
    "    X_non_numeric = pd.get_dummies(X[non_number_cols], drop_first=False)\n",
    "    # X_non_numeric = X[non_number_cols]\n",
    "    # for col in non_number_cols:\n",
    "    #     X_non_numeric[col] = LabelEncoder().fit_transform(X_non_numeric[col])\n",
    "\n",
    "    n_components = min(8, len(X_numeric.keys()))\n",
    "    if transforms is None:\n",
    "        std_scaler = StandardScaler()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_numeric = std_scaler.fit_transform(X_numeric)\n",
    "        # X_numeric = pca.fit_transform(X_numeric)\n",
    "    else:\n",
    "        std_scaler, pca = transforms\n",
    "        X_numeric = std_scaler.transform(X_numeric)\n",
    "        # X_numeric = pca.transform(X_numeric)\n",
    "\n",
    "    X_numeric_labels = [f\"pca_{i}\" for i in range(n_components)]\n",
    "    X_numeric = pd.DataFrame(X_numeric, columns=X_numeric_labels)\n",
    "    X = pd.concat([X_numeric, X_non_numeric], axis=1)\n",
    "\n",
    "    X = X.loc[:, ~X.columns.str.contains(\"unknown\")]\n",
    "\n",
    "    transforms = [std_scaler, pca]\n",
    "\n",
    "    if has_label:\n",
    "        y = df[\"outcome\"]\n",
    "        \n",
    "        return X, y, transforms\n",
    "    else:\n",
    "        if return_id:\n",
    "            return id, X, transforms\n",
    "        else:\n",
    "            return X, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, transforms = preprocessing(df_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=test_size, \n",
    "    shuffle=True, \n",
    "    random_state=random_seed,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPERPARAMETER SEARCH WITH RANDOM FOREST MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[y==0]), len(y[y==1]), len(y[y==1])/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val[y_val==0]), len(y_val[y_val==1]), len(y_val[y_val==1])/len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==1])/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set(X.keys())\n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Over Resampling minority class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.random import sample_without_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1 = X_train[y_train == 1], y_train[y_train == 1]\n",
    "X_train_0, y_train_0 = X_train[y_train == 0], y_train[y_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over sample minority class with a specific ratio\n",
    "ratio_scaler = 3\n",
    "oversample_ratio = 1\n",
    "ratio = (ratio_scaler * len(X_train_1))/len(X_train_0)\n",
    "ratio = 0.999 if ratio > 1.0 else ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, _, y_train_0, _ = train_test_split(X_train_0, y_train_0, train_size=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_0] + [X_train_1] * oversample_ratio)\n",
    "y_train = pd.concat([y_train_0] + [y_train_1] * oversample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train_0, X_train_1])\n",
    "# y_train = pd.concat([y_train_0, y_train_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over resample minority class so that minrity class are equal to majority class. Model using this step has lower performance\n",
    "# sm1 = SMOTE(random_state=random_seed)\n",
    "# X_train, y_train = sm1.fit_resample(X_train, y_train)\n",
    "# sm2 = SMOTE(random_state=random_seed)\n",
    "# X_val, y_val = sm2.fit_resample(X_val, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature selection and train classifier models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Train models with full features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_model = lambda: RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "# get_model = lambda: DecisionTreeClassifier(max_depth=100, random_state=random_seed)\n",
    "# get_model = lambda: AdaBoostClassifier(n_estimators=600, random_state=random_seed)\n",
    "# get_model = lambda: KNeighborsClassifier()\n",
    "get_model = lambda: GradientBoostingClassifier(n_estimators=150, max_depth=3, validation_fraction=0.01, random_state=1, learning_rate=0.05)\n",
    "# get_model = lambda: SVC(probability=True)\n",
    "# get_model = lambda: MLPClassifier(hidden_layer_sizes=500, max_iter=1000, verbose=False, learning_rate_init=1e-4, learning_rate=\"adaptive\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test models when choosing ALL available features:\n",
    "\n",
    "base_model = get_model()\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "#evaluate the accuracy on validation set\n",
    "y_pred = base_model.predict_proba(X_val)[:,1]\n",
    "auc_ = roc_auc_score(y_val, y_pred)\n",
    "print(auc_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot ROC curve*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_model1 = lambda: RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "# get_model2 = lambda: DecisionTreeClassifier(max_depth=100, random_state=random_seed)\n",
    "# get_model3 = lambda: AdaBoostClassifier(n_estimators=600, random_state=random_seed)\n",
    "# get_model4 = lambda: KNeighborsClassifier()\n",
    "# get_model5 = lambda: GradientBoostingClassifier(n_estimators=150, max_depth=3, validation_fraction=0.01, random_state=1, learning_rate=0.05)\n",
    "# get_model6 = lambda: MLPClassifier(hidden_layer_sizes=500, max_iter=1000, verbose=False, learning_rate_init=1e-4, learning_rate=\"adaptive\", early_stopping=True)\n",
    "\n",
    "\n",
    "# #random forest\n",
    "# rf_model = get_model1()\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# y_pred_rf = rf_model.predict_proba(X_val)[:,1]\n",
    "# auc_rf = roc_auc_score(y_val, y_pred_rf)\n",
    "\n",
    "# #decision tree\n",
    "# dt_model = get_model2()\n",
    "# dt_model.fit(X_train, y_train)\n",
    "# y_pred_dt = dt_model.predict_proba(X_val)[:,1]\n",
    "# auc_dt = roc_auc_score(y_val, y_pred_dt)\n",
    "\n",
    "# #adaboost\n",
    "# ada_model = get_model3()\n",
    "# ada_model.fit(X_train, y_train)\n",
    "# y_pred_ada = ada_model.predict_proba(X_val)[:,1]\n",
    "# auc_ada = roc_auc_score(y_val, y_pred_ada)\n",
    "\n",
    "# #Kneighbor\n",
    "# kn_model = get_model4()\n",
    "# kn_model.fit(X_train, y_train)\n",
    "# y_pred_kn = kn_model.predict_proba(X_val)[:,1]\n",
    "# auc_kn = roc_auc_score(y_val, y_pred_kn)\n",
    "\n",
    "# #Kneighbor\n",
    "# gb_model = get_model5()\n",
    "# gb_model.fit(X_train, y_train)\n",
    "# y_pred_gb = gb_model.predict_proba(X_val)[:,1]\n",
    "# auc_gb = roc_auc_score(y_val, y_pred_gb)\n",
    "\n",
    "# #Kneighbor\n",
    "# mlp_model = get_model5()\n",
    "# mlp_model.fit(X_train, y_train)\n",
    "# y_pred_mlp = mlp_model.predict_proba(X_val)[:,1]\n",
    "# auc_mlp = roc_auc_score(y_val, y_pred_mlp)\n",
    "\n",
    "# #plot ROC with AUC of each models\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# # Define the models and their names\n",
    "# models = [rf_model, dt_model, ada_model, kn_model, gb_model, mlp_model]\n",
    "# model_names = [\"Random Forest\", \"Decision Tree\", \"AdaBoost\", \"KNN\", \"Gradient Boosting\", \"MLP\"]\n",
    "# auc_scores = [auc_rf, auc_dt, auc_ada, auc_kn, auc_gb, auc_mlp]\n",
    "\n",
    "# # Plot the ROC curves\n",
    "# fig, ax = plt.subplots()\n",
    "# for i, model in enumerate(models):\n",
    "#     plot_roc_curve(model, X_val, y_val, ax=ax, name=model_names[i])\n",
    "\n",
    "# # Add labels and title\n",
    "# ax.set_xlabel(\"False Positive Rate\")\n",
    "# ax.set_ylabel(\"True Positive Rate\")\n",
    "# ax.set_title(\"ROC Curves of Different Models\")\n",
    "\n",
    "# # Add the AUC scores to the legend\n",
    "# ax.legend(loc=\"lower right\")\n",
    "# for i, model in enumerate(models):\n",
    "#     ax.text(1.0, 0.9-i*0.1, f\"{model_names[i]} (AUC={auc_scores[i]:.3f})\", transform=ax.transAxes, ha=\"right\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Features selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_selected_list = [\"age\", \"cconf\", \"employment\"]\n",
    "features_selected_list = []\n",
    "select_k = 1\n",
    "auc = 0\n",
    "it = 0\n",
    "max_iter = 50\n",
    "\n",
    "prev_selected = None\n",
    "\n",
    "pbar = tqdm(total=max_iter)\n",
    "while it < max_iter:\n",
    "    # select features and label\n",
    "    available_feature = list(features - set(features_selected_list))\n",
    "    if len(available_feature) == 0: break\n",
    "    if len(available_feature) > select_k:\n",
    "        new_feature = random.sample(available_feature, k=select_k)\n",
    "    else:\n",
    "        new_feature = random.sample(available_feature, k=1)\n",
    "    selected_features = list(features_selected_list) + new_feature\n",
    "\n",
    "    X_train_feature = X_train.loc[:, selected_features]\n",
    "    X_val_feature = X_val.loc[:, selected_features]\n",
    "\n",
    "    #train random forest classifier model with selected feature\n",
    "    model = get_model()\n",
    "    model.fit(X_train_feature, y_train)\n",
    "\n",
    "    #evaluate the accuracy on validation set\n",
    "    y_pred = model.predict_proba(X_val_feature)[:,1]\n",
    "    auc_ = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    if auc_ > auc:\n",
    "        features_selected_list.extend(new_feature)\n",
    "        print(f\"Feature '{new_feature}' is selected. AUC={auc_}\")\n",
    "        auc = auc_\n",
    "    # else:\n",
    "    #     print(f\"Feature '{new_feature}' is NOT selected.\")\n",
    "\n",
    "    it += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    # if len(features_selected_list) > 10:\n",
    "    #     break\n",
    "print(f\"Final AUC={auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Train models with selected features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test model when using selected features in features_selected_list\n",
    "X_train_feature = X_train.loc[:, features_selected_list]\n",
    "X_val_feature = X_val.loc[:, features_selected_list]\n",
    "\n",
    "#train random forest classifier model with selected feature\n",
    "model = get_model()\n",
    "model.fit(X_train_feature, y_train)\n",
    "\n",
    "#evaluate the accuracy on validation set\n",
    "y_pred = model.predict_proba(X_val_feature)[:,1]\n",
    "auc_ = roc_auc_score(y_val, y_pred)\n",
    "auc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_depth()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot ROC curve*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_model1 = lambda: RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "# get_model2 = lambda: DecisionTreeClassifier(max_depth=100, random_state=random_seed)\n",
    "# get_model3 = lambda: AdaBoostClassifier(n_estimators=600, random_state=random_seed)\n",
    "# get_model4 = lambda: KNeighborsClassifier()\n",
    "# get_model5 = lambda: GradientBoostingClassifier(n_estimators=150, max_depth=3, validation_fraction=0.01, random_state=1, learning_rate=0.05)\n",
    "# get_model6 = lambda: MLPClassifier(hidden_layer_sizes=500, max_iter=1000, verbose=False, learning_rate_init=1e-4, learning_rate=\"adaptive\", early_stopping=True)\n",
    "\n",
    "\n",
    "# #random forest\n",
    "# rf_model = get_model1()\n",
    "# rf_model.fit(X_train_feature, y_train)\n",
    "# y_pred_rf = rf_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_rf = roc_auc_score(y_val, y_pred_rf)\n",
    "\n",
    "# #decision tree\n",
    "# dt_model = get_model2()\n",
    "# dt_model.fit(X_train_feature, y_train)\n",
    "# y_pred_dt = dt_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_dt = roc_auc_score(y_val, y_pred_dt)\n",
    "\n",
    "# #adaboost\n",
    "# ada_model = get_model3()\n",
    "# ada_model.fit(X_train_feature, y_train)\n",
    "# y_pred_ada = ada_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_ada = roc_auc_score(y_val, y_pred_ada)\n",
    "\n",
    "# #Kneighbor\n",
    "# kn_model = get_model4()\n",
    "# kn_model.fit(X_train_feature, y_train)\n",
    "# y_pred_kn = kn_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_kn = roc_auc_score(y_val, y_pred_kn)\n",
    "\n",
    "# #Kneighbor\n",
    "# gb_model = get_model5()\n",
    "# gb_model.fit(X_train_feature, y_train)\n",
    "# y_pred_gb = gb_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_gb = roc_auc_score(y_val, y_pred_gb)\n",
    "\n",
    "# #Kneighbor\n",
    "# mlp_model = get_model5()\n",
    "# mlp_model.fit(X_train_feature, y_train)\n",
    "# y_pred_mlp = mlp_model.predict_proba(X_val_feature)[:,1]\n",
    "# auc_mlp = roc_auc_score(y_val, y_pred_mlp)\n",
    "\n",
    "# #plot ROC with AUC of each models\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# # Define the models and their names\n",
    "# models = [rf_model, dt_model, ada_model, kn_model, gb_model, mlp_model]\n",
    "# model_names = [\"Random Forest\", \"Decision Tree\", \"AdaBoost\", \"KNN\", \"Gradient Boosting\", \"MLP\"]\n",
    "# auc_scores = [auc_rf, auc_dt, auc_ada, auc_kn, auc_gb, auc_mlp]\n",
    "\n",
    "# # Plot the ROC curves\n",
    "# fig, ax = plt.subplots()\n",
    "# for i, model in enumerate(models):\n",
    "#     plot_roc_curve(model, X_val_feature, y_val, ax=ax, name=model_names[i])\n",
    "\n",
    "# # Add labels and title\n",
    "# ax.set_xlabel(\"False Positive Rate\")\n",
    "# ax.set_ylabel(\"True Positive Rate\")\n",
    "# ax.set_title(\"ROC Curves of Different Models\")\n",
    "\n",
    "# # Add the AUC scores to the legend\n",
    "# ax.legend(loc=\"lower right\")\n",
    "# for i, model in enumerate(models):\n",
    "#     ax.text(1.0, 0.9-i*0.1, f\"{model_names[i]} (AUC={auc_scores[i]:.3f})\", transform=ax.transAxes, ha=\"right\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USE TRAINED MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "id, X_test, _ = preprocessing(\n",
    "    df_test, \n",
    "    transforms=transforms, \n",
    "    has_label=False, \n",
    "    return_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing columns in the training test\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    X_test[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = base_model.predict(X_test)\n",
    "y_test_pred.sum()/len(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test.loc[:, features_selected_list])\n",
    "y_test_pred.sum()/len(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_test_pred = base_model.predict_proba(X_test)[:,1]\n",
    "y_test_pred = model.predict_proba(X_test.loc[:, features_selected_list])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise InterruptedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE CSV FILE WITH ID & PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_result_gbc_v14.csv\", \"w\") as f:\n",
    "    f.write(\"id,outcome\\n\")\n",
    "    for id_, y in zip(id, y_test_pred):\n",
    "        f.write(f\"{id_},{y}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d49c3f6d6dd49f9272b571d9fad348ab55b8c6c3f691520d74ed0af1f69c3dd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
